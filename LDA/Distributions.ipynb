{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distributions",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbgwZWWfWpp"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_S_umf2ukVl",
        "outputId": "b4325630-0585-47be-a10b-d6f6a2511dd4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAb77yZ9fzMG"
      },
      "source": [
        "import pandas as pd\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n",
        "import numpy as np\r\n",
        "import sys\r\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pafL7Li0jyXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac109d44-14b4-4e54-db55-31feb2508447"
      },
      "source": [
        "!pip install stop_words\r\n",
        "from stop_words import get_stop_words\r\n",
        "\r\n",
        "stop_words = get_stop_words('english')\r\n",
        "stop_words.append('said')\r\n",
        "all_words = stop_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop_words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp37-none-any.whl size=32917 sha256=7191a36099f9e6e079b1d1232827555d6491b18da694237846e50a328a3440e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ednaAOiRuk_q",
        "outputId": "02d6d2f3-f68e-4d68-eac4-e81c132eba5e"
      },
      "source": [
        "documents=[]\r\n",
        "for year in range(1981,2020):\r\n",
        "    i=0\r\n",
        "    file = pd.read_csv(f'/content/drive/MyDrive/articler_data/new_article_{year}.csv')\r\n",
        "    file = file.dropna().reset_index().drop('index',axis=1)\r\n",
        "#     file = file['article']\r\n",
        "#     file = file.dropna()\r\n",
        "    docs = []\r\n",
        "    for j in range(0, len(file)):\r\n",
        "        documents.append(file.loc[j,'article_cleaned_lemmatizer'])\r\n",
        "    print(year,end=', ')\r\n",
        "#     documents.append(docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_KaCZGxvCUG"
      },
      "source": [
        "n_features = 1000\r\n",
        "n_components = [5,10,15,20,25,30,40,50]\r\n",
        "n_top_words = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPZLeqyXvFJo",
        "outputId": "d274b142-f0a8-4368-f73f-407f30183334"
      },
      "source": [
        "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\r\n",
        "                                max_features=n_features,\r\n",
        "                                stop_words=all_words)\r\n",
        "tf = tf_vectorizer.fit_transform(documents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aren', 'can', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'mustn', 're', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2nBqLXVHKt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "195cd3b6-4cdd-4804-9319-c670562a0906"
      },
      "source": [
        "for n in n_components:\r\n",
        "  lda = LatentDirichletAllocation(n_components=n, max_iter=5,\r\n",
        "                                learning_method='online',\r\n",
        "                                learning_offset=50.,\r\n",
        "                                random_state=0)\r\n",
        "\r\n",
        "  lda.fit(tf)\r\n",
        "  distribution = lda.transform(tf)\r\n",
        "  dist = pd.DataFrame(distribution)\r\n",
        "  dist.to_csv(f'dist_{n}.csv',index=False)\r\n",
        "  print(n, 'done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40 done\n",
            "50 done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OnVEuM5H6sW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "708da3fd-60de-41e7-a1a3-56671a5ce706"
      },
      "source": [
        "n_topics = [5,10,15,20,25,30,40,50]\r\n",
        "no_years = 2020 - 1981\r\n",
        "\r\n",
        "for no in n_topics:\r\n",
        "  distribution = pd.read_csv(f'/content/dist_{no}.csv')\r\n",
        "  weights = np.zeros((no_years, no))\r\n",
        "  start = 0\r\n",
        "  for i in range(1981, 2020):\r\n",
        "    file = pd.read_csv(f'/content/drive/MyDrive/articler_data/new_article_{i}.csv')\r\n",
        "    year = file.dropna()\r\n",
        "\t\tyear_data = distribution[start:start+len(year),:]\r\n",
        "    yearly_sum = np.sum(year_data, axis = 0)/ len(year)\r\n",
        "\t\tweights[i - 1981] = yearly_sum\r\n",
        "\t\tstart = start + len(year)\r\n",
        "  wts_df = pd.DataFrame(weights)\r\n",
        "  wts_df.to_csv(f'/content/drive/MyDrive/weights/weights_{no}.csv',index=False)\r\n",
        "  print(no, 'done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TabError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-6134296a1169>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    year_data = distribution[start:start+len(year),:]\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_viCUFYNt61"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}